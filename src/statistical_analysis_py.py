# -*- coding: utf-8 -*-
"""statistical_analysis.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q77qWI212dUqp6BzlSRYQ55AxMN3Vriw
"""

#Mount to google drive
from google.colab import drive
drive.mount('/content/drive')

!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz

!tar xf spark-3.0.1-bin-hadoop2.7.tgz

#Setting enviroment varibles
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop2.7"

!pip install sympy

!pip install findspark

#Locating spark so can be use in google colab
import findspark# py -m pip install findspark
findspark.init()
print(findspark.find())

#createing spark session
from pyspark.sql import SparkSession
spark = SparkSession.builder\
        .master("local")\
        .appName("Statistical_Analysis")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

spark #outputs spark session details

#Accessing data
#path: /content/drive/MyDrive/UNI/Year 3/Full Unit Project/Data/sensor-fault-detection.csv
df = spark.read.csv("sensor-fault-detection.csv", header = True, inferSchema = True)
df.show(5) # data not in ideal format.

from sympy.ntheory import factorint
#formating data
#rdd = df.rdd #complicates the rdd set interms of formatting: includes row  structure
rdd = spark.sparkContext.textFile("sensor-fault-detection.csv")
#output
list = rdd.take(5)
print("Formatting...\n",list)

#formatting
rdd = rdd.map(lambda x: x.split(";"))

#remove first. For this case can do this way, no other row than the first filters
header = rdd.first()
rdd = rdd.filter(lambda x: x != header)

#convert 'SensorId' to int & 'Value' to in float
rdd = rdd.map(lambda x: (x[0],int(x[1]),float(x[2])))

#output
list = rdd.take(5)
print("Typeing...\nResult:\n",list)

#converting rdd to df 
df = rdd.toDF(header)# cant name the 4th column as i get an error
df.show(5)

"""Now that the data in 'sensor-fault-detection.csv' has been formated we can now perform statistical analysis. Inorder to do so I am going to make use of sparks mllib module."""

#Performing sttistical analyis on 'Value' columin in df before frequency disribuion
#using describe to find min, max, mean, standard deviation, count and range
from pyspark.sql import Row

statistics = df.describe(['Value'])

#calculateing range
min = statistics.filter(statistics['summary'] == 'min').collect()
max = statistics.filter(statistics['summary'] == 'max').collect()

range = spark.createDataFrame([('range', float(max[0].Value) - float(min[0].Value),)],['summary','Value'])

# output unoin of range and statistics
statistics = statistics.union(range)
statistics.show()

#Frequecy distribution total
from pyspark.sql.functions import asc,desc,col
print("showing top 10 values:")
top = df.groupBy('Value').count().orderBy(col('count').desc()).show(10)

# Commented out IPython magic to ensure Python compatibility.
#Frequency display the distribution graphically!
print('Displaying frequency distribution....')
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from pyspark.sql.functions import collect_list

#calculateing frequency distribution
frequency = df.groupBy('Value').count().orderBy(col('Value'))
frequency.show()
desc_df = df.groupBy('Value').count().orderBy(col('Value').desc())
desc_df.show()
#calculteing frequency stats - results in a slight change in  the mean- thats wy i decieded to show both
frequency_stats = frequency.describe(["Value"]).union(range)
frequency_stats.show()

#getting data from columns and removeing null values
data = frequency.select(collect_list('count')).first()[0]
data.pop(0)
labels = frequency.select(collect_list('Value')).first()[0]
labels.pop(0)  
plt.style.use('seaborn-white')
plt.title('Frequency Distribution')
plt.xlabel('Values')
plt.ylabel('Count')
plt.xticks(np.arange(len(labels)), labels)
plt.ylim(0,50)

#note can not see =m to get the y axis to scale
plt.hist(data, bins = 50,histtype='stepfilled')

"""Explantion for different loops in next block.
Seperate loop for labels and data as this:

while x <= 1:
  aprox_percentile_descriptions.append((str(int(x*100)),frequency.approxQuantile('Value',[float(x)],0)[0]))
  data.append(aprox_percentile_descriptions[int(x*10)-1][1])
  labels.append(aprox_percentile_descriptions int(x*10)-1][0])
  x += 0.1

  outputs this:

  [9.944543839, 13.38778782, 16.55446625, 19.63542366, 22.71856499, 25.90029716, 28.98782158, 28.98782158, 35.26742554, 35.26742554]

['10', '20', '30', '40', '50', '60', '70', '70', '89', '89']

  
"""

#Frequency distribution at fixed percentiles
aprox_percentile_descriptions = []
# graphial representation 
data = []
labels = []

'''
approxQuantile- calculate percetile
*x-A SparkDataFrame.
*col - The name of the numerical column.
*probabilities-A list of quantile probabilities. Each number must belong to [0, 1].
 For example 0 is the minimum, 0.5 is the median, 1 is the maximum.
*relativeError-The relative target precision to achieve (>= 0). If set to zero, 
 the exact quantiles are computed, which could be very expensive. Note that values
 greater than 1 are accepted but give the same result as 1.

'''
#exspensive
#calculateing percentiles
x = 0.1
while x <= 1:
  aprox_percentile_descriptions.append((str(int(x*100)),frequency.approxQuantile('Value',[float(x)],0)[0]))
  #data.append(aprox_percentile_descriptions[int(x*10)-1][1])
  #labels.append(aprox_percentile_descriptions[int(x*10)-1][0])
  x += 0.1

percentiles = spark.createDataFrame(aprox_percentile_descriptions,["Percentile (th)","Value"])
percentiles.show()

#setting up lables & data for visualisation

for i in aprox_percentile_descriptions:
  data.append(i[1]) 
  labels.append(i[0]) 

print(aprox_percentile_descriptions)
print(data)
print(labels)

#createing graphical representation
plt.style.use('seaborn-white')
plt.title('Fixed Percentiles')
plt.xlabel('Percentile')
plt.ylabel('Value')
plt.xticks(np.arange(len(labels)), labels)
plt.bar(np.arange(len(labels)),data, align = "center", alpha = 0.5)

import time
#Grouped Frequency Analysis 
#converting frequency distribution data frame into an rdd 
frequency_rdd = frequency.rdd.map(tuple)

global_distribution_data = []
j = 0
start = time.clock()
for i in data:
  print(i)
  #filtering values with in the class width
  group_distribution = frequency_rdd.filter(lambda x: x[0] <= i and x[0] > j)
 
  #key value pair (1, value frequency)
  group_distribution = group_distribution.map(lambda x: (1,x[1]))
  #print(group_distribution.take(5))
  #reduce by key adding all value frequencies with in a class width
  group_distribution = group_distribution.reduceByKey(lambda x,y:x+y)
  #print(group_distribution.take(5))
  group = group_distribution.collect()
  
  #storing results in an array for visualisation
  global_distribution_data.append(group[0][1])
  j = i

print("Processor time: ",time.clock() - start) 
print(global_distribution_data)

#Visualising the grouped Frequency distribution
labels = [round(i,2) for i in data]
plt.style.use('seaborn-white')
plt.title('Grouped Frequency Distribution')
plt.xlabel('Percentile')
plt.ylabel('Value')
plt.xticks(np.arange(len(labels)), labels)
plt.bar(np.arange(len(labels)),global_distribution_data, align = "center", alpha = 0.5)

#Median
median_val = percentiles.select(percentiles["Value"]).where(percentiles["Percentile (th)"] == 50).collect()
median = spark.createDataFrame([("median", median_val[0].Value)])
#median.show()
frequency_stats = frequency_stats.union(median)
frequency_stats.show()

#TDD- NO FUNCTIONS
import unittest

class Test_Statistical_Analysis(unittest.TestCase):
  
if __name__ == "__main__":
  suite = unittest.TestLoader().loadTestsFromTestCase(Test_Statistical_Analysis)
  runner = unittest.TextTestRunner(verbosity=2)
  runner.run(suite)